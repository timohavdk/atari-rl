{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c9ed8e-8fed-484a-85ea-c1efcbf7655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## УДАЛИТЬ ЯЧЕЙКУ!!! ИНАЧЕ БУДЕТ БЕЗ GPU!!!\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e57015-9523-4642-b471-060caa33ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:11:36.125469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-15 15:11:36.142640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-15 15:11:36.142669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-15 15:11:36.153981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-15 15:11:37.012271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium.utils.save_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528a1402-a137-4afa-b373-4ad6bc623692",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:11:38.465528: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-03-15 15:11:38.465562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: gentoo\n",
      "2025-03-15 15:11:38.465570: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: gentoo\n",
      "2025-03-15 15:11:38.465710: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 550.107.2\n",
      "2025-03-15 15:11:38.465733: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 550.107.2\n",
      "2025-03-15 15:11:38.465739: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:248] kernel version seems to match DSO: 550.107.2\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149d11a9-ca16-4258-ba7d-b0201080094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed813ff2-724f-4daa-a891-db80e2d279c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make_vec('LunarLander-v3', render_mode='rgb_array', num_envs=batch_size)\n",
    "valid_env = gym.make_vec('LunarLander-v3', render_mode='rgb_array', num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3564eb55-62ac-4849-a4c8-520d90605079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(prefix: str | None = None, suffix: str | None = None, separator: str = '_') -> str | None:\n",
    "    return prefix and prefix + separator + suffix or suffix or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59264df4-e173-40b2-8a03-a900ac13bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size: int, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, seed: int | None = None):\n",
    "        \"\"\"Stores the replay history with a maximum of `max_size` entries, removing old entries as needed.\n",
    "\n",
    "        Parameters:\n",
    "            max_size: maximal number of entries to keep\n",
    "            observation_space: specification of the observation space\n",
    "            action_space: specification of the action space\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "\n",
    "        self.current_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((max_size, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((max_size, *action_space.shape), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size,), dtype=np.float32)\n",
    "        \n",
    "        self.max_size = max_size\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0        \n",
    "        \n",
    "    def add(self, current_observations: np.ndarray, actions: np.ndarray, rewards: np.ndarray, next_observations: np.ndarray, dones: np.ndarray) -> None:\n",
    "        \"\"\"Add a new entry to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            current_observations: environment state observed at the current step\n",
    "            actions: action taken by the model\n",
    "            rewards: reward received after taking the action\n",
    "            next_observations: environment state obversed after taking the action\n",
    "            dones: whether the episode has ended or not\"\"\"\n",
    "        batch_size = current_observations.shape[0]\n",
    "        idxs = (np.arange(batch_size) + self.buffer_pointer) % self.max_size\n",
    "\n",
    "        self.current_observations[idxs] = current_observations\n",
    "        self.actions[idxs] = actions\n",
    "        self.rewards[idxs] = rewards\n",
    "        self.next_observations[idxs] = next_observations\n",
    "        self.dones[idxs] = dones\n",
    "\n",
    "        self.buffer_pointer = (idxs[-1] + 1) % self.max_size\n",
    "        self.current_size = min(self.max_size, self.current_size + batch_size)\n",
    "        \n",
    "    def sample(self, n_samples: int, replace: bool = True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Randomly samples `n_samples` from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "            n_samples: number of samples to select\n",
    "            replace: sample with or without replacement\n",
    "\n",
    "        Returns:\n",
    "            current observations, actions, rewards, next observations, done\"\"\"\n",
    "        return self[self.rng.choice(self.current_size, size=n_samples, replace=replace)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clears the buffer\"\"\"\n",
    "        self.buffer_pointer = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def __getitem__(self, index: int | np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Gets a sample at `index`\n",
    "\n",
    "        Parameters:\n",
    "            index: index of the sample to get\n",
    "\n",
    "        Returns:\n",
    "            current observation, action, reward, next observation, done\"\"\"\n",
    "        return (\n",
    "            self.current_observations[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.next_observations[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of entries in the buffer\"\"\"\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6106ac8c-1482-42a9-bfcb-87d3ca2a084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    observation_space: gym.spaces.Space,\n",
    "    action_space: gym.spaces.Space,\n",
    "    features: int,\n",
    "    blocks: int, \n",
    "    activation: str | keras.layers.Activation | None = 'silu',\n",
    "    dropout_rate: float = 0.,\n",
    "    multiply_freq: int = 1,\n",
    "    name: str | None = None\n",
    ") -> keras.Model:\n",
    "    \"\"\"Creates a simple MLP model\n",
    "\n",
    "    Arguments:\n",
    "        observation_space: model input vector size\n",
    "        features: initial model embedding size\n",
    "        action_space: model output vector size\n",
    "        block: number of perceptron layers\n",
    "        activation: intermediate model activation\n",
    "        dropout_rate: nuff said\n",
    "        multifly_freq: doubles embedding size every `multiply_freq` blocks\n",
    "        name: model name\n",
    "\n",
    "    Returns:\n",
    "        A MLP model\n",
    "    \"\"\"\n",
    "    inputs = x = keras.layers.Input(observation_space.shape, name=get_name(name, \"inputs\"), dtype=observation_space.dtype)\n",
    "\n",
    "    for i in range(blocks):\n",
    "        x = keras.layers.Dense(features, activation=activation, name=get_name(name, f\"dense_{i}\"))(x)\n",
    "        \n",
    "        if dropout_rate > 0.:\n",
    "            x = keras.layers.Dropout(dropout_rate, name=get_name(name, f\"dropout_{i}\"))(x)\n",
    "\n",
    "        if multiply_freq > 0 and (i + 1) % multiply_freq == 0:\n",
    "            features *= 2\n",
    "    \n",
    "    out = keras.layers.Dense(action_space.n, name=get_name(name, \"output\"))(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=out, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f35e32-99bc-4547-b5e5-34ea73311c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoss(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, gamma: float, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, q_current: keras.KerasTensor, q_next: keras.KerasTensor, rewards: keras.KerasTensor, actions: keras.KerasTensor, dones: keras.KerasTensor) -> keras.KerasTensor:\n",
    "        q_ref = rewards + self.gamma * (1 - dones) * keras.ops.max(q_next, axis=-1)\n",
    "\n",
    "        # return keras.ops.mean(\n",
    "        #     keras.ops.square(keras.ops.sum(q_current * actions, axis=-1) - q_ref)\n",
    "        # )\n",
    "        return keras.ops.square(keras.ops.sum(q_current * actions, axis=-1) - q_ref)\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super().get_config()\n",
    "        config['gamma'] = self.gamma\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66ae93b-6cf9-4bcf-8b81-dc4f47b51fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_model(\n",
    "    observation_space: gym.spaces.Space,\n",
    "    action_space: gym.spaces.Space,    \n",
    "    features: int,\n",
    "    blocks: int, \n",
    "    activation: str | keras.layers.Activation | None = 'silu',\n",
    "    dropout_rate: float = 0.,\n",
    "    multiply_freq: int = 1,\n",
    "    name: str | None = None,\n",
    "    gamma: float = 0.99\n",
    ") -> keras.Model:\n",
    "    \"\"\"Creates a combined model for Q-learning training\n",
    "\n",
    "    Arguments:\n",
    "        input_features: model input vector size\n",
    "        features: initial model embedding size\n",
    "        out_features: model output vector size\n",
    "        block: number of perceptron layers\n",
    "        activation: intermediate model activation\n",
    "        dropout_rate: nuff said\n",
    "        multifly_freq: doubles embedding size every `multiply_freq` blocks\n",
    "        name: model name,\n",
    "        gamma: rewards discount\n",
    "    Returns:\n",
    "        A q-model, a target q-model, a combined model\n",
    "    \"\"\"\n",
    "    model = get_model(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        features=features,\n",
    "        blocks=blocks,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        multiply_freq=multiply_freq,\n",
    "        name=get_name(name, \"model\"),\n",
    "    )\n",
    "    \n",
    "    target_model = get_model(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        features=features,\n",
    "        blocks=blocks,\n",
    "        activation=activation,\n",
    "        multiply_freq=multiply_freq,\n",
    "        name=get_name(name, \"target_model\"),\n",
    "    )\n",
    "\n",
    "    target_model.trainable = False\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    current_observation = keras.layers.Input(observation_space.shape, dtype=observation_space.dtype, name=get_name(name, \"curr_observ\"))\n",
    "    next_observation = keras.layers.Input(observation_space.shape, dtype=observation_space.dtype, name=get_name(name, \"next_observ\"))\n",
    "    current_action = keras.layers.Input((action_space.n,), dtype=\"float32\", name=get_name(name, \"curr_action\"))\n",
    "    rewards = keras.layers.Input((), dtype=\"float32\", name=get_name(name, \"rewards\"))\n",
    "    dones = keras.layers.Input((), dtype=\"float32\", name=get_name(name, \"dones\"))\n",
    "\n",
    "    model_res = model(current_observation)\n",
    "    target_model_res = target_model(next_observation)\n",
    "\n",
    "    loss = QLoss(gamma=gamma, name=get_name(name, \"Q_loss\"))(model_res, target_model_res, rewards, current_action, dones)\n",
    "\n",
    "    return (\n",
    "        model, \n",
    "        target_model, \n",
    "        keras.Model(\n",
    "            inputs=[\n",
    "                current_observation,\n",
    "                current_action,\n",
    "                rewards,\n",
    "                next_observation,\n",
    "                dones,\n",
    "            ],\n",
    "            outputs=loss,\n",
    "            name=name\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b8a4f6b-cb5c-40f3-a322-868b27145d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def __init__(self, epsilon: float, seed: int | None = None, greedy: bool = False):\n",
    "        \"\"\"Selects a random action with probability `epsilon` otherwise selects the most probably action given by the model.\n",
    "\n",
    "        Parameters:\n",
    "            epsilon: the probability to select a random action\n",
    "            seed: seed to initialize the internal random number generator for reproducibility\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.greedy = greedy\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    \n",
    "    def _sample(self, probabilities: np.ndarray) -> np.ndarray:\n",
    "        cumsum = np.cumsum(probabilities, axis=-1)\n",
    "        probs = self.rng.uniform(size=(cumsum.shape[0], 1))\n",
    "        msk = cumsum > probs\n",
    "        return np.argmax(msk, axis=-1)\n",
    "\n",
    "        \n",
    "    def __call__(self, probabilities: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action given the `probabilities\n",
    "\n",
    "        Parameters:\n",
    "            probabilities: probabilities for each action\n",
    "\n",
    "        Returns:\n",
    "            index of the selected action\"\"\"\n",
    "        batch, *_ = probabilities.shape\n",
    "        probs = self.rng.uniform(size=batch)\n",
    "\n",
    "        return np.where(\n",
    "            probs < self.epsilon,\n",
    "            self.rng.choice(probabilities.shape[1], size=batch),\n",
    "            np.argmax(probabilities, axis=-1) if self.greedy else self._sample(probabilities),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927b391e-1976-414a-9022-e55348bc4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(\n",
    "    model: keras.Model,\n",
    "    buffer: ReplayBuffer,\n",
    "    env: gym.Env,\n",
    "    steps: int,\n",
    "    sampler: Sampler,\n",
    "    observations: np.ndarray | None = None,\n",
    "    one_episode: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Plays the environment `env` using model `model` for a total of `steps` steps.\n",
    "\n",
    "    Arguments:\n",
    "        model: model to use\n",
    "        buffer: buffer to store results to\n",
    "        env: environment to use\n",
    "        steps: total number of steps to record\n",
    "        sampler: sampler to use\n",
    "        observations: observation to start from\n",
    "        one_episode: exist as soon as one of the environments finishes\n",
    "\n",
    "    Returns:\n",
    "        the last observations\n",
    "    \"\"\"\n",
    "    if observations is None:\n",
    "        observations, _ = env.reset()\n",
    "\n",
    "    for _ in range(steps // env.num_envs):\n",
    "        res = model(observations, training=False).numpy()\n",
    "        actions = sampler(res)\n",
    "        new_observations, rewards, terminateds, truncated, _ = env.step(actions) # new_observations, rewards, terminateds, truncated, info\n",
    "\n",
    "        dones = terminateds | truncated\n",
    "\n",
    "        buffer.add(\n",
    "            current_observations=observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            next_observations=new_observations,\n",
    "            dones=dones,\n",
    "        )\n",
    "        \n",
    "        if one_episode and np.any(dones):\n",
    "            observations = None\n",
    "            break\n",
    "        \n",
    "        observations = new_observations\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01514b6b-9ba3-4ec0-950f-8ccf4fa37131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDataset(keras.utils.PyDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps_per_epoch: int,\n",
    "        batch_size: int,\n",
    "        buffer: ReplayBuffer,\n",
    "        action_space: gym.spaces.Space,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.buffer = buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self._eye = np.eye(action_space.n)\n",
    "        self._answer = np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        a, b, c, d, e = self.buffer.sample(self.batch_size)\n",
    "        b = self._eye[b]\n",
    "\n",
    "        return (a, b, c, d, e), self._answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9961468-84a3-4280-b81c-d6a95bfc8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelUpdateCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, frequency: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def on_train_batch_end(self, batch: int, logs=None):\n",
    "        if (batch + 1) % self.frequency == 0:\n",
    "            self.model.get_layer(\n",
    "                f'{self.model.name}_target_model'\n",
    "            ).set_weights(\n",
    "                self.model.get_layer(\n",
    "                    f'{self.model.name}_model'\n",
    "                ).get_weights()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b909947-9625-454b-a193-85f3c92a88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvalCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, env: gym.Env, max_steps: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.env = env\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.buffer = ReplayBuffer(\n",
    "            max_size=max_steps,\n",
    "            observation_space=env.single_observation_space,\n",
    "            action_space=env.single_observation_space,\n",
    "        )\n",
    "\n",
    "        self.sampler = Sampler(epsilon=0., greedy=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        model = self.model.get_layer(\n",
    "            f'{self.model.name}_model'\n",
    "        )\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "        play_game(\n",
    "            model=model,\n",
    "            buffer=self.buffer,\n",
    "            env=self.env,\n",
    "            steps=self.max_steps,\n",
    "            sampler=self.sampler,\n",
    "            one_episode=True,\n",
    "        )\n",
    "\n",
    "        rewards = self.buffer.rewards[:len(self.buffer)].reshape((-1, self.env.num_envs))\n",
    "        logs['score'] = np.mean(np.sum(rewards, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027cc643-406f-45a3-888c-37cf023b5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SometimesPlayCallback(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        buffer: ReplayBuffer,\n",
    "        sampler: Sampler,\n",
    "        frequency: int,\n",
    "        steps_per_play: int,\n",
    "        epsilon_decay: keras.optimizers.schedules.PolynomialDecay | None = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.env = env\n",
    "        self.buffer = buffer\n",
    "        self.sampler = sampler\n",
    "        self.frequency = frequency\n",
    "        self.steps_per_play = steps_per_play\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.observations = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if self.epsilon_decay is not None:\n",
    "            self.sampler.epsilon = self.epsilon_decay(epoch)\n",
    "        if epoch == 0:\n",
    "            model = self.model.get_layer(\n",
    "                f'{self.model.name}_model'\n",
    "            )\n",
    "\n",
    "            self.buffer.clear()\n",
    "            \n",
    "            self.observations = play_game(\n",
    "                model=model,\n",
    "                buffer=self.buffer,\n",
    "                env=self.env,\n",
    "                steps=self.steps_per_play,\n",
    "                sampler=self.sampler,\n",
    "                observations=self.observations\n",
    "            )\n",
    "\n",
    "    def on_train_batch_start(self, batch_num: int, logs=None):\n",
    "        if (batch_num + 1) % self.frequency:\n",
    "            return\n",
    "        \n",
    "        model = self.model.get_layer(\n",
    "            f'{self.model.name}_model'\n",
    "        )\n",
    "\n",
    "        self.observations = play_game(\n",
    "            model=model,\n",
    "            buffer=self.buffer,\n",
    "            env=self.env,\n",
    "            steps=self.steps_per_play,\n",
    "            sampler=self.sampler,\n",
    "            observations=self.observations\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93f19cc-08e3-418a-a274-2c4c0c996329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, target_model, combined_model = get_combined_model(\n",
    "    observation_space=train_env.single_observation_space,\n",
    "    action_space=train_env.single_action_space,\n",
    "    features=64,\n",
    "    blocks=4,\n",
    "    activation='silu',\n",
    "    dropout_rate=0.2,\n",
    "    multiply_freq=1,\n",
    "    name='lander',\n",
    "    gamma=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed19d647-8e61-4963-b2f3-641803daa966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lander\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"lander\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lander_curr_observ  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_next_observ  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_model        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">175,556</span> │ lander_curr_obse… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_target_model │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">175,556</span> │ lander_next_obse… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_rewards      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_curr_action  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_dones        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_Q_loss       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lander_model[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QLoss</span>)             │                   │            │ lander_target_mo… │\n",
       "│                     │                   │            │ lander_rewards[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ lander_curr_acti… │\n",
       "│                     │                   │            │ lander_dones[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ lander_curr_observ  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_next_observ  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_model        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │    \u001b[38;5;34m175,556\u001b[0m │ lander_curr_obse… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_target_model │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │    \u001b[38;5;34m175,556\u001b[0m │ lander_next_obse… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_rewards      │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_curr_action  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_dones        │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lander_Q_loss       │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ lander_model[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mQLoss\u001b[0m)             │                   │            │ lander_target_mo… │\n",
       "│                     │                   │            │ lander_rewards[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ lander_curr_acti… │\n",
       "│                     │                   │            │ lander_dones[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">351,112</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m351,112\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">175,556</span> (685.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m175,556\u001b[0m (685.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">175,556</span> (685.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m175,556\u001b[0m (685.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f01b410-787f-44d1-9062-db17b2722ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2 ** 15     # 32_768\n",
    "steps_per_play = 2 ** 11  # 2048\n",
    "steps_per_epoch = 2 ** 10 # 1024\n",
    "\n",
    "end_epsilon = 0.1\n",
    "epochs = 128\n",
    "decay_epochs = epochs // 2\n",
    "update_frequency = 512\n",
    "play_frequency = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7a4baa1-27ef-49e2-a5e2-a20926c0dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(1, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a6f75c8-d596-4118-924d-f8924513463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay = keras.optimizers.schedules.PolynomialDecay(1., decay_epochs, end_learning_rate=end_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30065ff9-d7a0-44b9-a331-c24921010ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buffer = ReplayBuffer(\n",
    "    max_size=buffer_size,\n",
    "    observation_space=train_env.single_observation_space,\n",
    "    action_space=train_env.single_action_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9fcd57-3b6f-4a3c-bbbc-d05e7fe2523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QDataset(\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    batch_size=train_env.num_envs,\n",
    "    buffer=train_buffer,\n",
    "    action_space=train_env.single_action_space\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f69317c-8317-48d0-b9ea-ca43c6f17c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelUpdateCallback(update_frequency),\n",
    "    ModelEvalCallback(valid_env, 1000),\n",
    "    SometimesPlayCallback(\n",
    "        env=train_env,\n",
    "        buffer=train_buffer,\n",
    "        sampler=sampler,\n",
    "        frequency=play_frequency,\n",
    "        steps_per_play=steps_per_play,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9921378b-3298-4615-a6d4-6333fdd7a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(\n",
    "    loss=(lambda y_true, y_pred: keras.ops.mean(y_pred)),\n",
    "    optimizer=keras.optimizers.Adam(1e-4, clipnorm=5, weight_decay=2e-5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15506d90-e1c3-4d3c-bc21-9048e3c253f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = play_game(\n",
    "    model=model,\n",
    "    buffer=train_buffer,\n",
    "    env=train_env,\n",
    "    steps=steps_per_play,\n",
    "    sampler=sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c112171f-40ee-437d-a02e-16ed8a16b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/excorador/source/nn/venv/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['lander_curr_observ', 'lander_curr_action', 'lander_rewards', 'lander_next_observ', 'lander_dones']. Received: the structure of inputs=('*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1.9953 - score: -447.7466\n",
      "Epoch 2/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1.3995 - score: -353.7941\n",
      "Epoch 3/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 1.3067 - score: -363.3596\n",
      "Epoch 4/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 1.4505 - score: -91.0697\n",
      "Epoch 5/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 1.6598 - score: -140.9352\n",
      "Epoch 6/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 1.7304 - score: -74.9289\n",
      "Epoch 7/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16ms/step - loss: 1.9158 - score: -118.2722\n",
      "Epoch 8/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 1.9586 - score: -139.3293\n",
      "Epoch 9/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 2.1048 - score: -89.8066\n",
      "Epoch 10/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 2.2878 - score: -63.7563\n",
      "Epoch 11/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - loss: 2.4656 - score: -127.3888\n",
      "Epoch 12/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 2.6994 - score: -128.8807\n",
      "Epoch 13/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 2.7776 - score: -84.8775\n",
      "Epoch 14/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - loss: 3.0141 - score: 107.8529\n",
      "Epoch 15/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 3.2516 - score: 193.0912\n",
      "Epoch 16/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 3.4075 - score: 196.4797\n",
      "Epoch 17/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 3.5253 - score: 179.3081\n",
      "Epoch 18/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.7305 - score: -106.1290\n",
      "Epoch 19/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.8220 - score: -47.2177\n",
      "Epoch 20/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 3.8839 - score: -11.0880\n",
      "Epoch 21/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.9778 - score: -58.1472\n",
      "Epoch 22/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.9742 - score: -72.2129\n",
      "Epoch 23/128\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 4.0232 - score: -94.1129\n",
      "Epoch 24/128\n",
      "\u001b[1m 126/1024\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3413"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcombined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:239\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     function_type_utils\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    231\u001b[0m         args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlookup_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_func_context\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_cache.py:50\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m   dispatch_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict[context]\u001b[38;5;241m.\u001b[39mdispatch(function_type)\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dispatch_type:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_primary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/source/nn/venv/lib/python3.12/site-packages/tensorflow/core/function/polymorphism/function_type.py:456\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems()), \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "combined_model.fit(dataset, callbacks=callbacks, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323b6be-f435-4ec8-bd7d-d481434a4cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
